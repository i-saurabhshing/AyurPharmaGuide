{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "# from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Text Preprocessing Function\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text.lower())\n",
        "    # Removing stopwords and non-alphabetic characters\n",
        "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "data = pd.read_csv('Symptom2Disease.csv')\n",
        "\n",
        "# Displaying the dataset\n",
        "\n",
        "# print(data)\n",
        "\n",
        "data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "\n",
        "# Concise summary of DataFrame\n",
        "\n",
        "# print(data.info())\n",
        "\n",
        "# Check for null values\n",
        "\n",
        "# print(data.isnull().sum())\n",
        "\n",
        "# Display column names\n",
        "\n",
        "# print(data.columns)\n",
        "\n",
        "# print(data.value_counts())\n",
        "\n",
        "# Extracting 'label' and 'text' columns from the 'data' DataFrame\n",
        "\n",
        "labels = data['label']  # Contains the labels or categories associated with the text data\n",
        "symptoms = data['text']  # Contains the textual data (e.g., symptoms, sentences) for analysis\n",
        "\n",
        "# Text Preprocessing\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Apply preprocessing to symptoms\n",
        "\n",
        "preprocessed_symptoms = symptoms.apply(preprocess_text)\n",
        "# print(preprocessed_symptoms)\n",
        "\n",
        "# Feature Extraction using TF-IDF\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000)  # You can adjust max_features based on your dataset size\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_symptoms).toarray()\n",
        "# print(f'{tfidf_vectorizer}\\n\\n{tfidf_features}')\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# KNN Model Training\n",
        "\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) based on your dataset\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Predictions\n",
        "\n",
        "predictions = knn_classifier.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# Model Evaluation\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "symptom = input(\"Enter the symptoms separated by comma: \")\n",
        "\n",
        "# Preprocess the input symptom\n",
        "preprocessed_symptom = preprocess_text(symptom)\n",
        "\n",
        "# Transform the preprocessed symptom using the same vectorizer used during training\n",
        "symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])\n",
        "\n",
        "# Predict the disease\n",
        "predicted_disease = knn_classifier.predict(symptom_tfidf)\n",
        "# print(preprocessed_symptom)\n",
        "print(f'Predicted Disease: {predicted_disease[0]}')\n",
        "\n",
        "\n",
        "# print(symptom_tfidf)\n",
        "\n",
        "data = pd.read_csv('ayurvedic_symptoms_desc_updated.csv')\n",
        "\n",
        "words = symptom.split(\",\")\n",
        "\n",
        "data['common_words'] = data['English_Symptoms'].apply(lambda x: sum(word.lower() in x.lower() for word in words))\n",
        "\n",
        "\n",
        "# Filter the data based on text similarity\n",
        "filtered_data = data[data['common_words'] > 0]\n",
        "\n",
        "# Sort the DataFrame based on the number of common words\n",
        "filtered_data = filtered_data.sort_values(by='common_words', ascending=False)\n",
        "\n",
        "# Drop the 'common_words' column as it's no longer needed\n",
        "filtered_data = filtered_data.drop(columns=['common_words'])\n",
        "\n",
        "original_data_same_indices = data.loc[filtered_data.index]\n",
        "\n",
        "# Print or return the data\n",
        "print(original_data_same_indices)\n",
        "############################################################################################################################\n",
        "original_data_same_indices = original_data_same_indices.head(10)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def get_column_values(df, column_name):\n",
        "    # Get the column values as a list\n",
        "    column_values = df[column_name].tolist()\n",
        "\n",
        "    # Convert the list to a string with space separation\n",
        "    column_values_str = ' '.join(map(str, column_values))\n",
        "\n",
        "    return column_values_str\n",
        "\n",
        "df1 = pd.read_csv('Formulation-Indications.csv')\n",
        "\n",
        "formulations_lst = list(df1['Name of Medicine'])\n",
        "\n",
        "original_list = list(df1['Main Indications'])\n",
        "\n",
        "processed_list = []\n",
        "\n",
        "for item in original_list:\n",
        "    # Remove spaces and newline characters, convert to lowercase\n",
        "    processed_item = ''.join(item.split()).lower()\n",
        "    processed_list.append(processed_item)\n",
        "\n",
        "# print(processed_list[:5])\n",
        "\n",
        "# List of lists of symptoms\n",
        "list_of_symptoms = processed_list\n",
        "\n",
        "# Flatten the list of lists and split the symptoms using commas and spaces\n",
        "flat_symptoms = [symptom.replace(',', ' ').split() for symptoms in list_of_symptoms for symptom in symptoms.split(',')]\n",
        "\n",
        "# Get unique symptoms as a list\n",
        "unique_symptoms = list(set(symptom for sublist in flat_symptoms for symptom in sublist))\n",
        "\n",
        "# Print the unique symptoms\n",
        "# print(unique_symptoms[:5])\n",
        "\n",
        "data = {\n",
        "    \"Formulation\": formulations_lst,\n",
        "    \"Symptoms\": processed_list,\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "symptoms = pd.read_csv('ayurvedic_symptoms_desc_updated.csv')\n",
        "\n",
        "symptoms['Symptom'] = symptoms['Symptom'].str.lower()\n",
        "\n",
        "def symptoms_desc(symptom_name):\n",
        "    row = symptoms[symptoms['Symptom'] == symptom_name.lower()]\n",
        "#     print(row)\n",
        "    if not row.empty:\n",
        "        description = row.iloc[0]['Description']\n",
        "        print(f'Description of \"{symptom_name}\": {description}')\n",
        "    else:\n",
        "        print(f'Symptom \"{symptom_name}\" not found in the DataFrame.')\n",
        "\n",
        "def symptoms_lst_desc(user_symptoms):\n",
        "    for item in user_symptoms:\n",
        "#         print(item)\n",
        "        symptoms_desc(item)\n",
        "\n",
        "import difflib\n",
        "\n",
        "# Your list of correct words (assuming you have a list called unique_symptoms)\n",
        "correct_words = unique_symptoms\n",
        "\n",
        "def correct_symptoms(symptoms):\n",
        "    corrected_symptoms = []\n",
        "    for symptom in symptoms:\n",
        "        corrected_symptom = difflib.get_close_matches(symptom, correct_words, n=1, cutoff=0.6)\n",
        "        if corrected_symptom:\n",
        "            corrected_symptoms.append(corrected_symptom[0])\n",
        "        else:\n",
        "            corrected_symptoms.append(symptom)\n",
        "    return corrected_symptoms\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "data = {\n",
        "    \"Formulation\": formulations_lst,\n",
        "    \"Symptoms\": processed_list,\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform the symptom text data into numerical features\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['Symptoms'])\n",
        "\n",
        "# Create and train a classifier (e.g., Naive Bayes)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_tfidf, df['Formulation'])\n",
        "\n",
        "# Spelling Correction\n",
        "user_input = get_column_values(original_data_same_indices, 'Symptom')\n",
        "print(user_input)\n",
        "input_symptoms = user_input.split()\n",
        "new_symptoms = correct_symptoms(input_symptoms)\n",
        "print(f\"Did you mean: {', '.join(new_symptoms)}\")\n",
        "\n",
        "# Find Symptom Description\n",
        "symptoms_lst_desc(new_symptoms)\n",
        "\n",
        "# Predict Formulation\n",
        "new_symptoms_tfidf = tfidf_vectorizer.transform(new_symptoms)\n",
        "predicted_label = clf.predict(new_symptoms_tfidf)\n",
        "print(f\"Predicted Formulation: {predicted_label[0]}\")\n",
        "\n",
        "c = len(original_data_same_indices) if len(original_data_same_indices)<10 else 10\n",
        "\n",
        "while (c>0):\n",
        "  print(\"*\"*50)\n",
        "  ### Create a boolean mask to filter rows where the second column matches any element in closest_formulations\n",
        "  mask = df1.iloc[:, 0].isin([predicted_label[len(original_data_same_indices)-c]])\n",
        "\n",
        "  # Use the mask to select the rows that match the condition\n",
        "  filtered_df = df1[mask]\n",
        "\n",
        "  # Iterate through the filtered DataFrame and print each row separately\n",
        "  for index, row in filtered_df.iterrows():\n",
        "      print(row)\n",
        "  print(\"*\"*50)\n",
        "  c-=1"
      ],
      "metadata": {
        "id": "14xGk2oTvWK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/PROJECT_DA_FILES/')"
      ],
      "metadata": {
        "id": "2TMWu8jMOFJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ieUHB6n8_9Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "tMguILW_viWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "qGg7Zxf5vr7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "GieiNjqkvz5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZoJbvOBs9klS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}